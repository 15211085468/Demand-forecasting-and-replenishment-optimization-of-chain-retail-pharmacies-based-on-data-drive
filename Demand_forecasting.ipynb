{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding : utf-8\n",
    "# author : 欧宁益\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "# import os\n",
    "# import is_holiday\n",
    "import requests\n",
    "import json\n",
    "# import calendar \n",
    "import torch\n",
    "import pickle  \n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "# from typing import Any, Dict\n",
    "import jieqi\n",
    "import lunarcalendar as lc\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer,Baseline,NHiTS,NBeats\n",
    "from pytorch_forecasting.data import NaNLabelEncoder,GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE,MultivariateNormalDistributionLoss,MQF2DistributionLoss\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "# sns.set_theme(font='Microsoft YaHei')\n",
    "# warnings.filterwarnings('ignore')\n",
    "import chinese_calendar as cc\n",
    "from chinese_calendar.solar_terms import (\n",
    "    SOLAR_TERMS_C_NUMS,\n",
    "    SOLAR_TERMS_DELTA,\n",
    "    SOLAR_TERMS_MONTH,\n",
    "    SolarTerms,\n",
    ")\n",
    "from lightning.pytorch.tuner import Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "base_path = 'c:/OuNingyi/21级工程管理欧宁益/论文基础数据/'\n",
    "sale_df = pd.read_csv(base_path + 'sale_data.csv')\n",
    "shop_info = pd.read_csv(base_path + 'shop_info.csv')\n",
    "goods_info = pd.read_csv(base_path + 'goods_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据清洗和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节气计算函数\n",
    "def get_solar_terms(date):\n",
    "    \"\"\"\n",
    "    生成 24 节气\n",
    "    通用寿星公式： https://www.jianshu.com/p/1f814c6bb475\n",
    "\n",
    "    通式寿星公式：[Y×D+C]-L\n",
    "    []里面取整数； Y=年数的后2位数； D=0.2422； L=Y/4，小寒、大寒、立春、雨水的 L=(Y-1)/4\n",
    "\n",
    "    该函数由chinese_calendar的get_solar_terms函数基础上进行改进\n",
    "    \"\"\"    \n",
    "    year, month = date.year, date.month\n",
    "    if not 1900 <= year <= 2100:\n",
    "        raise NotImplementedError(\"only year between [1900, 2100] supported\")\n",
    "    D = 0.2422\n",
    "    result = []\n",
    "    # 按月计算节气\n",
    "    for solar_term in SOLAR_TERMS_MONTH[month]:\n",
    "        nums = SOLAR_TERMS_C_NUMS[solar_term]\n",
    "        C = nums[0] if year < 2000 else nums[1]\n",
    "        # 2000 年的小寒、大寒、立春、雨水按照 20 世纪的 C 值来算\n",
    "        if year == 2000 and solar_term in [\n",
    "            SolarTerms.lesser_cold,\n",
    "            SolarTerms.greater_cold,\n",
    "            SolarTerms.the_beginning_of_spring,\n",
    "            SolarTerms.rain_water,\n",
    "        ]:\n",
    "            C = nums[0]\n",
    "        Y = year % 100\n",
    "        L = int(Y / 4)\n",
    "        if solar_term in [\n",
    "            SolarTerms.lesser_cold,\n",
    "            SolarTerms.greater_cold,\n",
    "            SolarTerms.the_beginning_of_spring,\n",
    "            SolarTerms.rain_water,\n",
    "        ]:\n",
    "            L = int((Y - 1) / 4)\n",
    "        day = int(Y * D + C) - L\n",
    "        # 计算偏移量\n",
    "        delta = SOLAR_TERMS_DELTA.get((year, solar_term))\n",
    "        if delta:\n",
    "            day += delta\n",
    "        _date = datetime.date(year, month, day)\n",
    "        if date.day == _date.day:\n",
    "            return solar_term.value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为时间格式\n",
    "sale_df.ds = pd.to_datetime(sale_df.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成销售日期列表\n",
    "date_list = []\n",
    "current_date = sale_df.ds.min()\n",
    "while current_date <= sale_df.ds.max():\n",
    "    date_list.append(current_date)  # 格式化日期为YYYYMMDD形式存入列表\n",
    "    current_date += datetime.timedelta(days=1)  # 增加一天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame(date_list,columns=['ds'])\n",
    "\n",
    "time_df['year'] = time_df['ds'].dt.year\n",
    "time_df['month'] = time_df['ds'].dt.month\n",
    "time_df['day'] = time_df['ds'].dt.day\n",
    "time_df['weekday'] = time_df['ds'].dt.weekday\n",
    "time_df['is_weekend'] = time_df.weekday.map(lambda x:1 if x>=5 else 0)\n",
    "# 计算本年度第几周\n",
    "time_df['week_of_year'] = time_df['ds'].dt.isocalendar().week.astype(int)\n",
    "# 计算本年第几天\n",
    "time_df['day_of_year'] = time_df['ds'].dt.dayofyear\n",
    "# 计算季度\n",
    "time_df['quarter'] = time_df['ds'].dt.quarter\n",
    "# 是否休息日\n",
    "time_df['is_holiday'] = time_df.ds.map(lambda x:cc.is_holiday(x)*1)\n",
    "# 节日具体名称\n",
    "time_df['holiday_name'] = time_df.ds.map(lambda x:cc.get_holiday_detail(x)[1]).fillna(0)\n",
    "# 农历日期\n",
    "time_df['lunar_date'] = time_df.ds.map(lambda x:lc.Converter.Solar2Lunar(lc.Solar(x.year,x.month,x.day)))\n",
    "time_df['lunar_year'] = time_df.lunar_date.map(lambda x:x.year)\n",
    "time_df['lunar_month'] = time_df.lunar_date.map(lambda x:x.month)\n",
    "time_df['lunar_day'] = time_df.lunar_date.map(lambda x:x.day)\n",
    "time_df['lunar_is_leap'] = time_df.lunar_date.map(lambda x:x.isleap*1) # 是否闰月\n",
    "# 计算节气\n",
    "time_df['solar_terms'] = time_df.ds.map(lambda x:get_solar_terms(x)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选有效数据超过60%的部分\n",
    "day_cnt = sale_df[sale_df.sale_qty>0].groupby(['shop_code','goods_code']).sale_qty.count().rename('天数').reset_index()\n",
    "sale_left_05 = day_cnt.loc[day_cnt.天数 > sale_df.ds.nunique()*0.5,['shop_code','goods_code']]\n",
    "sale_df2 = sale_df.merge(sale_left_05,how='inner',on=['shop_code','goods_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 门店商品和日期的笛卡尔积\n",
    "sale_df3 = pd.merge(sale_df2[['shop_code','goods_code']].drop_duplicates(), time_df.drop(columns='lunar_date'), how='cross').merge(sale_df2,how='left',on=['shop_code','goods_code','ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 空值用0替换\n",
    "sale_df3.sale_amt.fillna(0, inplace=True)\n",
    "sale_df3.sale_qty.fillna(0, inplace=True)\n",
    "# 将销售量为负的，用0进行替换\n",
    "sale_df3.loc[sale_df3.sale_qty < 0,'sale_qty'] = 0\n",
    "sale_df3.loc[sale_df3.sale_amt < 0,'sale_amt'] = 0\n",
    "# 增加时间索引\n",
    "sale_df3['time_idx'] = sale_df3.groupby(['shop_code','goods_code']).ds.rank('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算去年同期值\n",
    "sale_df3['sale_qty_yoy'] = sale_df3.sale_qty.shift(365)\n",
    "sale_df3['sale_amt_yoy'] = sale_df3.sale_amt.shift(365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计特征\n",
    "sale_df3['log_sale_qty'] = sale_df3.sale_qty.apply(lambda x:np.log(x+ 1e-8)) # log压缩数值范围\n",
    "sale_df3['avg_sale_qty_by_shop_sku'] = sale_df3.groupby(['shop_code','goods_code'],observed=True).sale_qty.transform('mean')\n",
    "sale_df3['avg_sale_qty_by_sku'] = sale_df3.groupby('goods_code',observed=True).sale_qty.transform('sum') / sale_df3.time_idx.max()\n",
    "sale_df3['avg_sale_qty_by_shop'] = sale_df3.groupby('shop_code',observed=True).sale_qty.transform('sum') / sale_df3.time_idx.max()\n",
    "\n",
    "sale_df3['log_sale_amt'] = sale_df3.sale_amt.apply(lambda x:np.log(x+ 1e-8)) # log压缩数值范围\n",
    "sale_df3['avg_sale_amt_by_shop_sku'] = sale_df3.groupby(['shop_code','goods_code'],observed=True).sale_amt.transform('mean')\n",
    "sale_df3['avg_sale_amt_by_sku'] = sale_df3.groupby('goods_code',observed=True).sale_amt.transform('sum') / sale_df3.time_idx.max()\n",
    "sale_df3['avg_sale_amt_by_shop'] = sale_df3.groupby('shop_code',observed=True).sale_amt.transform('sum') / sale_df3.time_idx.max()\n",
    "# 去年同期值空值用平均值填充\n",
    "sale_df3['sale_qty_yoy'].fillna(sale_df3['avg_sale_qty_by_shop_sku'],inplace=True)\n",
    "sale_df3['sale_amt_yoy'].fillna(sale_df3['avg_sale_amt_by_shop_sku'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换格式\n",
    "for i in sale_df3.select_dtypes(include=['float64']).columns:\n",
    "    sale_df3[i] = sale_df3[i].astype('float32')\n",
    "for j in sale_df3.select_dtypes(include=['int64']).columns:\n",
    "    sale_df3[j] = sale_df3[j].astype(str).astype('int32')\n",
    "for k in sale_df3.select_dtypes(include=['object']).columns:\n",
    "    sale_df3[k] = sale_df3[k].astype(str).astype('category')\n",
    "for l in ['shop_code','goods_code']+['is_weekend','is_holiday','holiday_name','lunar_is_leap','solar_terms']:\n",
    "    sale_df3[l] = sale_df3[l].astype(str).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 店铺信息处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取format_name字段唯一值\n",
    "format_col_name = shop_info['format_name'].str.split(',', expand=True).stack().unique()\n",
    "# 设置format_col_name对应的字段名称\n",
    "format_is_fieldate = ['is_support_commercial_insurance','is_general_pharmacy','is_support_remote_medical_insurance_settlement','is_support_chronic_disease','is_contain_convenience_area',\n",
    "                    'is_E_commerce_virtual_pharmacy','is_outpatient_coordination_pharmacy','is_DTP_pharmacy']\n",
    "# 对format_name进行拆分\n",
    "for k,v in zip(format_col_name, format_is_fieldate):\n",
    "    shop_info[v] = shop_info['format_name'].apply(lambda x:1 if k in x else 0)\n",
    "\n",
    "# 获取busi_district_type_name字段唯一值\n",
    "busi_district_type_col_name = shop_info['busi_district_type_name'].str.split(',', expand=True).stack().unique()\n",
    "# 设置busi_district_type_col_name对应的字段名称\n",
    "busi_district_type_is_fieldate = ['is_hosptial_pharmacy','is_community_pharmacy','is_business_district_adjacent_street_pharmacy','is_vegetable_market_pharmacy',\n",
    "                                'is_transportation_junction_pharmacy','is_business_district_shop_in_shop','is_tourist_attraction_pharmacy','is_tertiary_school_pharmacy',\n",
    "                                'is_business_district_pharmacy','is_primary_school_pharmacy','is_airport_railway_station_pharmacy','is_park_pharmacy']\n",
    "# 对busi_district_type_name进行拆分\n",
    "for k,v in zip(busi_district_type_col_name, busi_district_type_is_fieldate):\n",
    "    shop_info[v] = shop_info['busi_district_type_name'].apply(lambda x:1 if k in x else 0)   \n",
    "\n",
    "# 将已拆分字段弃置\n",
    "shop_prep = shop_info.drop(['format_name','busi_district_type_name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连续变量空值填充为中位数，并标记\n",
    "shop_prep['is_avg_sale_amt_na'] = 0\n",
    "shop_prep.loc[shop_prep.avg_sale_amt.isnull(),'is_avg_sale_amt_na'] = 1\n",
    "shop_prep.avg_sale_amt = shop_prep.avg_sale_amt.fillna(shop_prep.avg_sale_amt.median())\n",
    "\n",
    "# 筛选文本分类变量\n",
    "shop_str_list = shop_prep.select_dtypes(include=['object']).columns.to_list()\n",
    "\n",
    "# 标记分类变量\n",
    "shop_static_real_col = ['rental_area','use_area','store_area', 'busi_area', 'gd_lat', 'gd_lgt', 'dis_income','avg_sale_amt','month_age']\n",
    "shop_static_cate_col = shop_prep.columns.drop(shop_static_real_col).to_list()\n",
    "\n",
    "# 将分类变量转化类型\n",
    "for sscc in shop_static_cate_col:\n",
    "    shop_prep[sscc] = shop_prep[sscc].fillna(0).astype(str).astype('category')\n",
    "for sscc in shop_prep.select_dtypes(include=['int64']).columns:\n",
    "    shop_prep[sscc] = shop_prep[sscc].fillna(0).astype('int32')\n",
    "for sscc in shop_prep.select_dtypes(include=['float64']).columns:\n",
    "    shop_prep[sscc] = shop_prep[sscc].fillna(0).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 商品信息处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选文本分类变量\n",
    "goods_str_list = goods_info.select_dtypes(include=['object']).columns.to_list()\n",
    "\n",
    "# 识别分类变量\n",
    "goods_static_cate_col = goods_info.columns.to_list()\n",
    "\n",
    "# 格式转换\n",
    "for sscc2 in goods_static_cate_col:\n",
    "    goods_info[sscc2] = goods_info[sscc2].fillna(0).astype(str).astype('category')\n",
    "for sscc2 in shop_prep.select_dtypes(include=['int64']).columns:\n",
    "    goods_info[sscc2] = goods_info[sscc2].fillna(0).astype('int32')\n",
    "for sscc2 in shop_prep.select_dtypes(include=['float64']).columns:\n",
    "    goods_info[sscc2] = goods_info[sscc2].fillna(0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据合并\n",
    "main = sale_df3.merge(shop_prep,on='shop_code').merge(goods_info,on='goods_code')\n",
    "main.dis_income = main.dis_income.astype('int32')\n",
    "main.month_age = main.month_age.astype('int32')\n",
    "main.time_idx = main.time_idx.astype('int32')\n",
    "for col in main.select_dtypes(include=['float64']).columns:\n",
    "    main[col] = main[col].astype('float32')\n",
    "# 用于训练和验证的数据\n",
    "sale_left_06 = day_cnt.loc[day_cnt.天数 > sale_df.ds.nunique()*0.6,['shop_code','goods_code']]\n",
    "sale_left_06.shop_code = sale_left_06.shop_code.astype(str)\n",
    "sale_left_06.goods_code = sale_left_06.goods_code.astype(str)\n",
    "main_ts = main.merge(sale_left_06,how='inner',on=['shop_code','goods_code'])\n",
    "# 用于后续预测需求进行深度学习的数据\n",
    "main_dl = main[~main.index.isin(main_ts.index)]\n",
    "# # 保存数据\n",
    "main_ts.to_csv(base_path + 'main_ts.csv',index=False)\n",
    "main_dl.to_csv(base_path + 'main_dl.csv',index=False)\n",
    "# 保存数据格式\n",
    "main_dtypes_dict = main.dtypes.drop(['ds','shop_code','goods_code']).to_dict()\n",
    "main_dtypes_dict_path = base_path + 'main_dtypes_dict.json'\n",
    "with open(main_dtypes_dict_path, 'wb') as f:  \n",
    "    pickle.dump(main_dtypes_dict, f)\n",
    "    pickle.dump(shop_static_cate_col, f)\n",
    "    pickle.dump(goods_static_cate_col, f) \n",
    "    pickle.dump(shop_static_real_col, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、TemporalFusionTransformer销售预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据格式\n",
    "base_path = 'c:/OuNingyi/21级工程管理欧宁益/论文基础数据/'\n",
    "main_dtypes_dict_path = base_path + 'main_dtypes_dict.json'\n",
    "with open(main_dtypes_dict_path, 'rb') as f:  \n",
    "    main_dtypes_dict = pickle.load(f)  \n",
    "    shop_static_cate_col = pickle.load(f) \n",
    "    goods_static_cate_col = pickle.load(f) \n",
    "    shop_static_real_col = pickle.load(f)\n",
    "# 读取主数据用于销售预测模型训练\n",
    "main = pd.read_csv(base_path + 'main_ts.csv', dtype=main_dtypes_dict,parse_dates=['ds'])\n",
    "main.shop_code = main.shop_code.astype(str).astype('category')\n",
    "main.goods_code = main.goods_code.astype(str).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.sales_scan_name.unique()\n",
    "# main = main[main.sales_scan_name == '小店']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima只能对单时间序列进行预测,故分组循环预测\n",
    "loss_list = []\n",
    "for _, group_df in main[['shop_code','goods_code','ds','sale_qty']].groupby(['shop_code','goods_code']):\n",
    "    arima = auto_arima(group_df['sale_qty'], seasonal=True)\n",
    "    sale_qty_predicted = arima.predict(n_periods=7)\n",
    "    # 计算损失\n",
    "    loss_list.append(SMAPE()(torch.from_numpy(sale_qty_predicted.values).reshape(-1, 1),torch.from_numpy(group_df.loc[group_df.ds > '2023-07-24','sale_qty'].values).reshape(-1, 1)))\n",
    "# 对各个时间序列的损失求均值\n",
    "torch.stack(loss_list).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一年预测7天\n",
    "max_encoder_length = 365\n",
    "max_prediction_length = 7\n",
    "\n",
    "context_length = max_encoder_length\n",
    "prediction_length = max_prediction_length\n",
    "\n",
    "training_cutoff = main[\"time_idx\"].max() - max_prediction_length\n",
    "# 部分数据训练集, 用于展示图形\n",
    "# main = main[main.goods_code.isin(['1020105787'])&(main.shop_code == '1215')]\n",
    "training_data = main[lambda x: (x.time_idx <=  main[\"time_idx\"].max()-max_prediction_length)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造符合模型的数据集\n",
    "training = TimeSeriesDataSet(\n",
    "    training_data,\n",
    "    time_idx = \"time_idx\",\n",
    "    target = \"sale_qty\",    \n",
    "    static_categoricals = shop_static_cate_col+goods_static_cate_col, # 门店信息和商品信息中的分类变量作为静态分类变量\n",
    "    static_reals = shop_static_real_col , # 门店信息和商品信息中的连续变量作为静态连续变量\n",
    "    time_varying_known_categoricals= ['is_weekend','is_holiday','holiday_name', 'lunar_is_leap','solar_terms'],\n",
    "    time_varying_known_reals = ['time_idx','year', 'month', 'day', 'weekday',  'week_of_year','day_of_year', 'quarter', 'lunar_year', 'lunar_month', 'lunar_day' ,'sale_qty_yoy','sale_amt_yoy',\n",
    "                                'avg_sale_qty_by_shop_sku','avg_sale_qty_by_sku','avg_sale_qty_by_shop','avg_sale_amt_by_shop_sku','avg_sale_amt_by_sku','avg_sale_amt_by_shop'],    \n",
    "    time_varying_unknown_reals = ['sale_qty','sale_amt','log_sale_qty','log_sale_amt'],\n",
    "    categorical_encoders = {label:NaNLabelEncoder(add_nan = True).fit(training_data[f'{label}']) for label in shop_static_cate_col + goods_static_cate_col}, # 对所有分类变量进行编码\n",
    "    group_ids = ['shop_code','goods_code'],\n",
    "    max_encoder_length = context_length,\n",
    "    min_encoder_length = context_length,\n",
    "    max_prediction_length = prediction_length,\n",
    "    min_prediction_length = prediction_length,\n",
    "    target_normalizer = GroupNormalizer(groups=['shop_code','goods_code'], transformation=\"softplus\"),\n",
    "    add_relative_time_idx = True,\n",
    "    add_target_scales = True,\n",
    "    add_encoder_length = True,\n",
    "    allow_missing_timesteps = True\n",
    ")\n",
    "validation = TimeSeriesDataSet.from_dataset(training, main, predict=True, stop_randomization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置精度\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# 数据加载\n",
    "batch_size = 128 \n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=8,persistent_workers=True,pin_memory=True)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=8,persistent_workers=True,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算基准误差\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)]).to(torch.device('cuda:0'))\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "SMAPE()(baseline_predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造符合模型的NBeats和NHiTS的数据集\n",
    "training_N = TimeSeriesDataSet(\n",
    "    training_data,\n",
    "    time_idx = \"time_idx\",\n",
    "    target = \"sale_qty\",    \n",
    "    # static_categoricals = shop_static_cate_col+goods_static_cate_col, # 门店信息和商品信息中的分类变量作为静态分类变量\n",
    "    # static_reals = shop_static_real_col , # 门店信息和商品信息中的连续变量作为静态连续变量\n",
    "    # time_varying_known_categoricals= ['is_weekend','is_holiday','holiday_name', 'lunar_is_leap','solar_terms'],\n",
    "    # time_varying_known_reals = ['time_idx','year', 'month', 'day', 'weekday',  'week_of_year','day_of_year', 'quarter', 'lunar_year', 'lunar_month', 'lunar_day' ,'sale_qty_yoy','sale_amt_yoy',\n",
    "    #                             'avg_sale_qty_by_shop_sku','avg_sale_qty_by_sku','avg_sale_qty_by_shop','avg_sale_amt_by_shop_sku','avg_sale_amt_by_sku','avg_sale_amt_by_shop'],    \n",
    "    time_varying_unknown_reals = ['sale_qty'],\n",
    "    categorical_encoders = {label:NaNLabelEncoder(add_nan = True).fit(training_data[f'{label}']) for label in shop_static_cate_col + goods_static_cate_col}, # 对所有分类变量进行编码\n",
    "    group_ids = ['shop_code','goods_code'],\n",
    "    max_encoder_length = context_length,\n",
    "    min_encoder_length = context_length,\n",
    "    max_prediction_length = prediction_length,\n",
    "    min_prediction_length = prediction_length,\n",
    "    target_normalizer = GroupNormalizer(groups=['shop_code','goods_code'], transformation=\"softplus\"),\n",
    "    add_relative_time_idx = False,\n",
    ")\n",
    "validation_N = TimeSeriesDataSet.from_dataset(training, main, predict=True, stop_randomization=True,min_prediction_idx=training_cutoff + 1)\n",
    "# 设置精度\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# 数据加载\n",
    "batch_size = 128 \n",
    "train_dataloader_N = training_N.to_dataloader(train=True, batch_size=batch_size)\n",
    "val_dataloader_N = validation_N.to_dataloader(train=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBeats参数寻优及训练\n",
    "pl.seed_everything(24)\n",
    "trainer_NBeats = pl.Trainer(accelerator=\"gpu\", gradient_clip_val=1e-1)\n",
    "net_NBeats = NBeats.from_dataset(training_N, learning_rate=3e-2, weight_decay=1e-2, widths=[32, 512], backcast_loss_ratio=0.1)\n",
    "# find optimal learning rate\n",
    "res_NBeats = Tuner(trainer_NBeats).lr_find(net_NBeats, train_dataloaders=train_dataloader_N, val_dataloaders=val_dataloader_N, min_lr=1e-5)\n",
    "print(f\"suggested learning rate: {res_NBeats.suggestion()}\")\n",
    "fig = res_NBeats.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "net_NBeats.hparams.learning_rate = res_NBeats.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "trainer_NBeats = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.01,\n",
    "    callbacks=[early_stop_callback],\n",
    "    limit_train_batches=150,\n",
    ")\n",
    "\n",
    "\n",
    "net_NBeats = NBeats.from_dataset(\n",
    "    training_N,\n",
    "    learning_rate=0.022387211385683406,\n",
    "    log_interval=10,\n",
    "    log_val_interval=1,\n",
    "    weight_decay=1e-2,\n",
    "    widths=[32, 512],\n",
    "    backcast_loss_ratio=1.0,\n",
    ")\n",
    "\n",
    "trainer_NBeats.fit(\n",
    "    net_NBeats,\n",
    "    train_dataloaders=train_dataloader_N,\n",
    "    val_dataloaders=val_dataloader_N,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBeats验证集的误差\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)]).to(torch.device('cuda:0'))\n",
    "predictions = net_NBeats.predict(val_dataloader, return_y=True,trainer_kwargs=dict(accelerator=\"gpu\")) \n",
    "SMAPE()(predictions.output, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHiTS参数寻优及训练\n",
    "pl.seed_everything(24)\n",
    "trainer_NHiTS = pl.Trainer(accelerator=\"gpu\", gradient_clip_val=1e-1)\n",
    "net_NHiTS = NHiTS.from_dataset(training_N, learning_rate=3e-2, weight_decay=1e-2,  backcast_loss_ratio=0.1)\n",
    "# find optimal learning rate\n",
    "res_NHiTS = Tuner(trainer_NHiTS).lr_find(net_NHiTS, train_dataloaders=train_dataloader_N, val_dataloaders=val_dataloader_N, min_lr=1e-5)\n",
    "print(f\"suggested learning rate: {res_NHiTS.suggestion()}\")\n",
    "fig = res_NHiTS.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "net_NHiTS.hparams.learning_rate = res_NHiTS.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_NHiTS = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.01,\n",
    "    callbacks=[early_stop_callback],\n",
    "    limit_train_batches=150,\n",
    ")\n",
    "\n",
    "\n",
    "net_NHiTS = NHiTS.from_dataset(\n",
    "    training_N,\n",
    "    learning_rate=0.0031622776601683794,\n",
    "    log_interval=10,\n",
    "    log_val_interval=1,\n",
    "    weight_decay=1e-2,\n",
    "    backcast_loss_ratio=0.0,\n",
    "    hidden_size=64,\n",
    "    optimizer=\"AdamW\",\n",
    "    loss=MQF2DistributionLoss(prediction_length=max_prediction_length),\n",
    ")\n",
    "\n",
    "trainer_NHiTS.fit(\n",
    "    net_NHiTS,\n",
    "    train_dataloaders=train_dataloader_N,\n",
    "    val_dataloaders=val_dataloader_N,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHiTS验证集的误差\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)]).to(torch.device('cuda:0'))\n",
    "predictions = net_NHiTS.predict(val_dataloader, return_y=True,trainer_kwargs=dict(accelerator=\"gpu\")) \n",
    "SMAPE()(predictions.output, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动寻找最优超参数\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=200,\n",
    "    # gradient_clip_val_range=(0.01, 1.0),\n",
    "    # hidden_size_range=(8, 128),\n",
    "    # hidden_continuous_size_range=(8, 128),\n",
    "    # attention_head_size_range=(1, 4),\n",
    "    # learning_rate_range=(0.001, 0.1),\n",
    "    # dropout_range=(0.1, 0.3),\n",
    "    # trainer_kwargs=dict(limit_train_batches=1),\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "    \n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正式训练参数设置\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator=\"gpu\",    \n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=6.542287226245969,\n",
    "    limit_train_batches=50,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\"),\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.02818382931264452,\n",
    "    hidden_size=20,  \n",
    "    attention_head_size=1,\n",
    "    dropout=0.2611769602088634, \n",
    "    hidden_continuous_size=14, \n",
    "    loss=SMAPE(),\n",
    "    optimizer=\"Ranger\",\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4    \n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正式训练\n",
    "trainer.fit(\n",
    "    tft, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存最佳模型\n",
    "# best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_model_path = 'lightning_logs\\\\lightning_logs\\\\version_40\\\\checkpoints\\\\epoch=36-step=1850.ckpt'\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集的误差\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)]).to(torch.device('cuda:0'))\n",
    "predictions = best_tft.predict(val_dataloader, return_y=True,trainer_kwargs=dict(accelerator=\"gpu\")) \n",
    "SMAPE()(predictions.output, actuals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
